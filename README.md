# llm-txtfile-chatting

# make a python env
# install numpy , ollama and llama3 (you can use any llm just make the changes in the main.py file accordingly)
# that should help you run the project
# (this will run according to your cpu and gpu capabilities)
# commands

![Commands](https://github.com/hackice20/llm-txtfile-chatting/blob/main/Screenshot%202024-08-21%20143521.png)

# output

![output](https://github.com/hackice20/llm-txtfile-chatting/blob/main/Screenshot%202024-08-21%20143508.png)
